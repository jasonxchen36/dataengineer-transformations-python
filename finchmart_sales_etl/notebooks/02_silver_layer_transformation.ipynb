{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Layer - Data Cleansing and Enrichment\n",
    "## FinchMart Sales ETL Pipeline\n",
    "\n",
    "This notebook transforms Bronze layer data by:\n",
    "- Cleansing data (handling nulls, duplicates, incorrect timestamps)\n",
    "- Enriching with product reference data\n",
    "- Implementing incremental processing\n",
    "- Storing cleaned data in Delta Lake Silver layer\n",
    "\n",
    "**Architecture Decision:** Using Delta Lake merge operations for incremental processing with deduplication logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, when, coalesce, lit, \n",
    "    row_number, current_timestamp, round as spark_round\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Delta Lake support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"FinchMart-Silver-Layer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = \"/home/ubuntu/dataengineer-transformations-python/finchmart_sales_etl\"\n",
    "BRONZE_PATH = f\"{BASE_PATH}/data/bronze/sales_transactions\"\n",
    "SILVER_PATH = f\"{BASE_PATH}/data/silver/sales_transactions_clean\"\n",
    "PRODUCT_REF_PATH = f\"{BASE_PATH}/data/raw/Product_Table.csv\"\n",
    "\n",
    "print(f\"Bronze Layer Path: {BRONZE_PATH}\")\n",
    "print(f\"Silver Layer Path: {SILVER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Bronze layer data\n",
    "bronze_df = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "print(f\"Bronze layer records: {bronze_df.count()}\")\n",
    "bronze_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Product reference data\n",
    "product_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(PRODUCT_REF_PATH)\n",
    "\n",
    "# Rename columns to avoid conflicts\n",
    "product_df = product_df \\\n",
    "    .withColumnRenamed(\"product_id\", \"prod_id\") \\\n",
    "    .withColumnRenamed(\"product_category\", \"ref_category\") \\\n",
    "    .withColumnRenamed(\"price\", \"list_price\")\n",
    "\n",
    "print(f\"Product reference records: {product_df.count()}\")\n",
    "product_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Step 1: Handle timestamp conversion and validation\n",
    "# Convert string timestamps to proper timestamp type\n",
    "# Filter out records with invalid timestamps\n",
    "cleansed_df = bronze_df \\\n",
    "    .withColumn(\"transaction_timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\")) \\\n",
    "    .filter(col(\"transaction_timestamp\").isNotNull())\n",
    "\n",
    "print(f\"Records after timestamp validation: {cleansed_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Step 2: Remove duplicates\n",
    "# Deduplication based on transaction_id, keeping the latest ingestion\n",
    "window_spec = Window.partitionBy(\"transaction_id\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "\n",
    "deduplicated_df = cleansed_df \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "print(f\"Records after deduplication: {deduplicated_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing Step 3: Handle null values and data quality issues\n",
    "# - Fill missing payment methods with 'Unknown'\n",
    "# - Fill missing store locations with 'Unknown'\n",
    "# - Ensure quantity is positive\n",
    "# - Ensure price is positive\n",
    "quality_df = deduplicated_df \\\n",
    "    .withColumn(\"payment_method\", coalesce(col(\"payment_method\"), lit(\"Unknown\"))) \\\n",
    "    .withColumn(\"store_location\", coalesce(col(\"store_location\"), lit(\"Unknown\"))) \\\n",
    "    .withColumn(\"quantity\", when(col(\"quantity\") > 0, col(\"quantity\")).otherwise(1)) \\\n",
    "    .withColumn(\"price\", when(col(\"price\") > 0, col(\"price\")).otherwise(0.0)) \\\n",
    "    .filter(col(\"transaction_id\").isNotNull()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull()) \\\n",
    "    .filter(col(\"product_id\").isNotNull())\n",
    "\n",
    "print(f\"Records after quality checks: {quality_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Enrichment: Join with product reference data\n",
    "enriched_df = quality_df \\\n",
    "    .join(product_df, quality_df.product_id == product_df.prod_id, \"left\") \\\n",
    "    .select(\n",
    "        col(\"transaction_id\"),\n",
    "        col(\"transaction_timestamp\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"product_name\"),\n",
    "        coalesce(col(\"ref_category\"), col(\"product_category\")).alias(\"product_category\"),\n",
    "        col(\"price\").alias(\"transaction_price\"),\n",
    "        col(\"list_price\"),\n",
    "        col(\"quantity\"),\n",
    "        (col(\"price\") * col(\"quantity\")).alias(\"total_amount\"),\n",
    "        col(\"payment_method\"),\n",
    "        col(\"store_location\"),\n",
    "        col(\"source_file\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_amount\", spark_round(col(\"total_amount\"), 2)) \\\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "print(f\"Records after enrichment: {enriched_df.count()}\")\n",
    "enriched_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Silver layer using Delta Lake\n",
    "# Using overwrite mode for initial load, but designed for incremental updates\n",
    "enriched_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(SILVER_PATH)\n",
    "\n",
    "print(\"Silver layer data written successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Silver layer data\n",
    "silver_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "print(f\"Total records in Silver layer: {silver_df.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "silver_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "silver_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Report for Silver Layer\n",
    "from pyspark.sql.functions import sum as _sum, avg, min as _min, max as _max\n",
    "\n",
    "print(\"=== Silver Layer Data Quality Report ===\")\n",
    "print(f\"Total transactions: {silver_df.count()}\")\n",
    "print(f\"Unique customers: {silver_df.select('customer_id').distinct().count()}\")\n",
    "print(f\"Unique products: {silver_df.select('product_id').distinct().count()}\")\n",
    "print(f\"Store locations: {silver_df.select('store_location').distinct().count()}\")\n",
    "\n",
    "# Statistical summary\n",
    "stats_df = silver_df.agg(\n",
    "    _sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "    _min(\"transaction_timestamp\").alias(\"earliest_transaction\"),\n",
    "    _max(\"transaction_timestamp\").alias(\"latest_transaction\")\n",
    ")\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "stats_df.show(truncate=False)\n",
    "\n",
    "# Category distribution\n",
    "print(\"\\nSales by Category:\")\n",
    "silver_df.groupBy(\"product_category\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_sales\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "# Store performance\n",
    "print(\"\\nSales by Store:\")\n",
    "silver_df.groupBy(\"store_location\") \\\n",
    "    .agg(_sum(\"total_amount\").alias(\"total_sales\")) \\\n",
    "    .orderBy(col(\"total_sales\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Processing Strategy\n",
    "\n",
    "For future incremental loads, use the following pattern:\n",
    "\n",
    "```python\n",
    "# Read new data from Bronze layer with watermark\n",
    "last_processed_timestamp = spark.read.format(\"delta\").load(SILVER_PATH) \\\n",
    "    .agg(_max(\"ingestion_timestamp\")).first()[0]\n",
    "\n",
    "new_bronze_df = spark.read.format(\"delta\").load(BRONZE_PATH) \\\n",
    "    .filter(col(\"ingestion_timestamp\") > last_processed_timestamp)\n",
    "\n",
    "# Apply same transformations to new data\n",
    "# Use Delta Lake MERGE for upsert operations\n",
    "delta_table = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_enriched_df.alias(\"source\"),\n",
    "    \"target.transaction_id = source.transaction_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Silver Layer Transformation Complete:**\n",
    "- Timestamps validated and converted to proper format\n",
    "- Duplicates removed based on transaction_id\n",
    "- Null values handled appropriately\n",
    "- Data enriched with product reference information\n",
    "- Total amount calculated for each transaction\n",
    "- Ready for Gold layer aggregations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
