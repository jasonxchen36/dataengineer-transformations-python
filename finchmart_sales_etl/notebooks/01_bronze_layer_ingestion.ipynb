{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer - Raw Data Ingestion\n",
    "## FinchMart Sales ETL Pipeline\n",
    "\n",
    "This notebook ingests raw CSV sales transaction files using Spark Structured Streaming and stores them in Delta Lake format in the Bronze layer.\n",
    "\n",
    "**Architecture Decision:** Using Structured Streaming with `readStream` to simulate real-time ingestion of CSV files, treating each file as a micro-batch representing 5 minutes of sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Delta Lake support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"FinchMart-Bronze-Layer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Delta Lake configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = \"/home/ubuntu/dataengineer-transformations-python/finchmart_sales_etl\"\n",
    "RAW_DATA_PATH = f\"{BASE_PATH}/data/raw\"\n",
    "BRONZE_PATH = f\"{BASE_PATH}/data/bronze/sales_transactions\"\n",
    "BRONZE_CHECKPOINT = f\"{BASE_PATH}/data/bronze/checkpoints/sales_transactions\"\n",
    "\n",
    "print(f\"Raw Data Path: {RAW_DATA_PATH}\")\n",
    "print(f\"Bronze Layer Path: {BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for sales transactions\n",
    "# Explicit schema definition ensures data quality and type safety\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", StringType(), False),  # Will be converted to TimestampType in Silver\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"store_location\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Schema defined for sales transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from CSV files\n",
    "# Using maxFilesPerTrigger=1 to simulate real-time processing of each 5-minute batch\n",
    "raw_sales_stream = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(sales_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(f\"{RAW_DATA_PATH}/Mock_Sales_Data*.csv\")\n",
    "\n",
    "print(\"Streaming source configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata columns for data lineage and audit\n",
    "bronze_sales_stream = raw_sales_stream \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "print(\"Metadata columns added for data lineage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Bronze layer as Delta table\n",
    "# Using append mode to accumulate all raw data\n",
    "# Checkpointing ensures exactly-once processing semantics\n",
    "bronze_query = bronze_sales_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", BRONZE_CHECKPOINT) \\\n",
    "    .option(\"path\", BRONZE_PATH) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Bronze layer streaming query started\")\n",
    "print(f\"Query ID: {bronze_query.id}\")\n",
    "print(f\"Status: {bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the streaming query\n",
    "import time\n",
    "\n",
    "# Wait for processing to complete (adjust timeout as needed)\n",
    "timeout = 60  # seconds\n",
    "start_time = time.time()\n",
    "\n",
    "while bronze_query.isActive and (time.time() - start_time) < timeout:\n",
    "    print(f\"Query is active. Progress: {bronze_query.lastProgress}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "# Stop the query after processing\n",
    "bronze_query.stop()\n",
    "print(\"Bronze layer ingestion completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bronze layer data\n",
    "bronze_df = spark.read.format(\"delta\").load(BRONZE_PATH)\n",
    "\n",
    "print(f\"Total records in Bronze layer: {bronze_df.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "bronze_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "bronze_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"=== Bronze Layer Data Quality Report ===\")\n",
    "print(f\"Total transactions: {bronze_df.count()}\")\n",
    "print(f\"Distinct transaction IDs: {bronze_df.select('transaction_id').distinct().count()}\")\n",
    "print(f\"Date range: {bronze_df.selectExpr('min(timestamp)', 'max(timestamp)').first()}\")\n",
    "print(f\"Source files processed: {bronze_df.select('source_file').distinct().count()}\")\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "null_counts = bronze_df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in bronze_df.columns\n",
    "])\n",
    "\n",
    "print(\"\\nNull counts by column:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Bronze Layer Ingestion Complete:**\n",
    "- Raw CSV files ingested using Spark Structured Streaming\n",
    "- Data stored in Delta Lake format for ACID transactions\n",
    "- Metadata columns added for data lineage tracking\n",
    "- Checkpoint mechanism ensures exactly-once processing\n",
    "- Ready for Silver layer transformation and cleansing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
