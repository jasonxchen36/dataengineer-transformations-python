{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Layer - Business Aggregations\n",
    "## FinchMart Sales ETL Pipeline\n",
    "\n",
    "This notebook creates optimized, aggregated tables for Power BI consumption:\n",
    "- Daily sales aggregations\n",
    "- Store performance metrics\n",
    "- Top products by revenue\n",
    "- Customer spending behavior\n",
    "\n",
    "**Architecture Decision:** Using Z-Ordering and partitioning strategies to optimize query performance for analytical workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum as _sum, avg, count, max as _max, min as _min,\n",
    "    date_format, to_date, row_number, round as spark_round, current_timestamp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with Delta Lake support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"FinchMart-Gold-Layer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = \"/home/ubuntu/dataengineer-transformations-python/finchmart_sales_etl\"\n",
    "SILVER_PATH = f\"{BASE_PATH}/data/silver/sales_transactions_clean\"\n",
    "GOLD_DAILY_PATH = f\"{BASE_PATH}/data/gold/daily_sales\"\n",
    "GOLD_STORE_PATH = f\"{BASE_PATH}/data/gold/store_performance\"\n",
    "GOLD_PRODUCTS_PATH = f\"{BASE_PATH}/data/gold/top_products\"\n",
    "GOLD_CUSTOMERS_PATH = f\"{BASE_PATH}/data/gold/customer_spending\"\n",
    "\n",
    "print(f\"Silver Layer Path: {SILVER_PATH}\")\n",
    "print(f\"Gold Layer Paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Silver layer data\n",
    "silver_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "print(f\"Silver layer records: {silver_df.count()}\")\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daily Sales Aggregation\n",
    "Total sales per day across all stores and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily sales aggregation\n",
    "daily_sales_df = silver_df \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\"))) \\\n",
    "    .groupBy(\"transaction_date\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        _sum(\"quantity\").alias(\"total_items_sold\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "        count(col(\"customer_id\").distinct()).alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_sales\", spark_round(col(\"total_sales\"), 2)) \\\n",
    "    .withColumn(\"avg_transaction_value\", spark_round(col(\"avg_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "    .orderBy(\"transaction_date\")\n",
    "\n",
    "print(\"Daily Sales Aggregation:\")\n",
    "daily_sales_df.show(10, truncate=False)\n",
    "\n",
    "# Write to Gold layer with partitioning by date\n",
    "daily_sales_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(GOLD_DAILY_PATH)\n",
    "\n",
    "print(f\"Daily sales data written to {GOLD_DAILY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Store Performance Aggregation\n",
    "Total sales per store with daily breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create store performance aggregation\n",
    "store_performance_df = silver_df \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\"))) \\\n",
    "    .groupBy(\"transaction_date\", \"store_location\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        _sum(\"quantity\").alias(\"total_items_sold\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "        count(col(\"customer_id\").distinct()).alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_sales\", spark_round(col(\"total_sales\"), 2)) \\\n",
    "    .withColumn(\"avg_transaction_value\", spark_round(col(\"avg_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "    .orderBy(\"transaction_date\", col(\"total_sales\").desc())\n",
    "\n",
    "print(\"Store Performance Aggregation:\")\n",
    "store_performance_df.show(20, truncate=False)\n",
    "\n",
    "# Write to Gold layer with partitioning\n",
    "store_performance_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(GOLD_STORE_PATH)\n",
    "\n",
    "print(f\"Store performance data written to {GOLD_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top Products by Revenue\n",
    "Top 5 products by revenue with daily breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create top products aggregation\n",
    "products_df = silver_df \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\"))) \\\n",
    "    .groupBy(\"transaction_date\", \"product_id\", \"product_name\", \"product_category\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        avg(\"transaction_price\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_revenue\", spark_round(col(\"total_revenue\"), 2)) \\\n",
    "    .withColumn(\"avg_price\", spark_round(col(\"avg_price\"), 2))\n",
    "\n",
    "# Rank products by revenue within each day\n",
    "window_spec = Window.partitionBy(\"transaction_date\").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "top_products_df = products_df \\\n",
    "    .withColumn(\"revenue_rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"revenue_rank\") <= 5) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "    .orderBy(\"transaction_date\", \"revenue_rank\")\n",
    "\n",
    "print(\"Top 5 Products by Revenue (per day):\")\n",
    "top_products_df.show(20, truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "top_products_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"transaction_date\") \\\n",
    "    .save(GOLD_PRODUCTS_PATH)\n",
    "\n",
    "print(f\"Top products data written to {GOLD_PRODUCTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customer Spending Behavior\n",
    "Average transaction value per customer with spending patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer spending aggregation\n",
    "customer_spending_df = silver_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "        _sum(\"quantity\").alias(\"total_items_purchased\"),\n",
    "        _min(\"transaction_timestamp\").alias(\"first_purchase_date\"),\n",
    "        _max(\"transaction_timestamp\").alias(\"last_purchase_date\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_spent\", spark_round(col(\"total_spent\"), 2)) \\\n",
    "    .withColumn(\"avg_transaction_value\", spark_round(col(\"avg_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp()) \\\n",
    "    .orderBy(col(\"total_spent\").desc())\n",
    "\n",
    "print(\"Customer Spending Behavior:\")\n",
    "customer_spending_df.show(20, truncate=False)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nCustomer Spending Statistics:\")\n",
    "customer_spending_df.select(\n",
    "    avg(\"total_spent\").alias(\"avg_customer_lifetime_value\"),\n",
    "    _max(\"total_spent\").alias(\"max_customer_spent\"),\n",
    "    _min(\"total_spent\").alias(\"min_customer_spent\"),\n",
    "    avg(\"transaction_count\").alias(\"avg_transactions_per_customer\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "customer_spending_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(GOLD_CUSTOMERS_PATH)\n",
    "\n",
    "print(f\"Customer spending data written to {GOLD_CUSTOMERS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization\n",
    "Apply Z-Ordering and optimize Delta tables for query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Gold layer tables with Z-Ordering\n",
    "# Z-Ordering co-locates related information in the same set of files\n",
    "# This improves query performance by reducing the amount of data read\n",
    "\n",
    "print(\"Optimizing Daily Sales table...\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_DAILY_PATH}` ZORDER BY (transaction_date)\")\n",
    "\n",
    "print(\"Optimizing Store Performance table...\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_STORE_PATH}` ZORDER BY (transaction_date, store_location)\")\n",
    "\n",
    "print(\"Optimizing Top Products table...\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_PRODUCTS_PATH}` ZORDER BY (transaction_date, revenue_rank)\")\n",
    "\n",
    "print(\"Optimizing Customer Spending table...\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_CUSTOMERS_PATH}` ZORDER BY (customer_id)\")\n",
    "\n",
    "print(\"\\nAll Gold layer tables optimized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacuum old files (clean up old versions)\n",
    "# Note: In production, set retention period appropriately (default 7 days)\n",
    "print(\"Vacuuming Delta tables to remove old files...\")\n",
    "\n",
    "spark.sql(f\"VACUUM delta.`{GOLD_DAILY_PATH}` RETAIN 0 HOURS\")\n",
    "spark.sql(f\"VACUUM delta.`{GOLD_STORE_PATH}` RETAIN 0 HOURS\")\n",
    "spark.sql(f\"VACUUM delta.`{GOLD_PRODUCTS_PATH}` RETAIN 0 HOURS\")\n",
    "spark.sql(f\"VACUUM delta.`{GOLD_CUSTOMERS_PATH}` RETAIN 0 HOURS\")\n",
    "\n",
    "print(\"Vacuum completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Data for Power BI\n",
    "Export aggregated data to CSV/Parquet for Power BI consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Gold layer tables to CSV for Power BI\n",
    "EXPORT_PATH = f\"{BASE_PATH}/data/gold/powerbi_export\"\n",
    "\n",
    "print(\"Exporting data for Power BI...\")\n",
    "\n",
    "# Export Daily Sales\n",
    "daily_sales_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{EXPORT_PATH}/daily_sales\")\n",
    "\n",
    "# Export Store Performance\n",
    "store_performance_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{EXPORT_PATH}/store_performance\")\n",
    "\n",
    "# Export Top Products\n",
    "top_products_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{EXPORT_PATH}/top_products\")\n",
    "\n",
    "# Export Customer Spending\n",
    "customer_spending_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{EXPORT_PATH}/customer_spending\")\n",
    "\n",
    "# Also export Silver layer for detailed analysis\n",
    "silver_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{EXPORT_PATH}/transactions_detail\")\n",
    "\n",
    "print(f\"All data exported to {EXPORT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Gold Layer Aggregation Complete:**\n",
    "- Daily sales aggregations created with key metrics\n",
    "- Store performance analyzed with daily breakdown\n",
    "- Top 5 products by revenue identified per day\n",
    "- Customer spending behavior analyzed\n",
    "- Tables optimized with Z-Ordering for query performance\n",
    "- Data partitioned by date for efficient filtering\n",
    "- Data exported to CSV for Power BI consumption\n",
    "\n",
    "**Partitioning Strategy:**\n",
    "- Daily sales, store performance, and top products partitioned by `transaction_date`\n",
    "- Enables efficient date-range queries\n",
    "- Reduces data scanning for time-series analysis\n",
    "\n",
    "**Z-Ordering Strategy:**\n",
    "- Applied on frequently filtered columns\n",
    "- Improves query performance by 2-10x for typical analytical queries\n",
    "- Co-locates related data for better data skipping"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
